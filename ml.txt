Week - 1:

1. import pandas as pd
   df = pd.read_csv('')			//READ CSV
   print(df)
2. rows, cols = df.shape		//DIMENSIONS
3. df.head(5)				//TOP 5 AND TOTAL DATA
4. df.columns				//COLUMNS
5. df.rename(columns = {'Temperature': 'Temp'},inplace = True)   //REPLACE COLUMN NAME
6. df.[['Date', 'Temp']]			//DISPLAY SINGLE COLUMN
7. df3 = pd.concat([df1,df2]).reset_index(drop = True)		//BIND ROW
8. df4 = pd.concat([df1,df2], axis= 1).reset_index(drop = True)		//BIND COLUMN
9. df.isnull()


Week - 2:

1. df['Temperature'].mean() , df['Temperature'].median(), df.['Temperature'].mode() 	//CENTRAL TENDENCY
2. df.describe()							//MEASURES OF DATA SPREAD
3. df['Temperature'].var()   df.['Temperature'].std()						//VARIANCE  && STANDARD DEVIATION
4. q1  = np.percentile(df['Temperature'], 25, interpolation = 'midpoint')		
   q2  = np.percentile(df['Temperature'], 50, interpolation = 'midpoint')	//QUARITLES, INTERQUARTILE RANGE
   q3  = np.percentile(df['Temperature'], 75, interpolation = 'midpoint')
   IQR = q3-q1

Week - 3:

1. import matplot.lib.pyplot as plt 
   import seaborn as sns
   sns.boxplot(X = 'Species', y= 'Weight', data = df) Wing, Tail, Weight are the predictors	//BOX PLOT FOR 4 PREDICTORS
   plt.show()
2. sns.boxplot(x = df['feature_name']) 
   sns.despine()
   plt.title(" ") plt.xlabel(" ") plt.ylabel(" ")					//BOXPLOT FOR SINGLE FEATURE
   plt.show()
3. sns.histplot(data = df, x='Weight')						//HISTOGRAM
   plt.show()	
4. df.plot(kind = 'scatter', x = 'Tail', y = 'Wing')			
   plt.grid() 									//SCATTER PLOT
   //sns.scatterplot(x = 'sepal_length', y = 'petal_length', data = df)//

Week - 4:

1. for col in df.columns:
  	df[col].replace('?', None, inplace = True)
   df.isnull().sum()

   i) max = df['Weight'].quantile(0.95)
      min = df['Weight'].quantile(0.05)
      df[(df['Weight']>max) | (df['Weight']<min)]  //PERCENTILES

   ii) df.Tail.mean()
       df.Tail.std()
       max = df.Tail.mean() + 3*df.Tail.std() 
       min = df.Tail.mean() - 3*df.Tail.std()  // STANDARD DEVIATION 
       print the df using the same logic	    

   iii) df['zsore'] = (df.Tail - df.Tail.mean()) / df.Tail
        df.head(5) 
        df[df['zsore']>0.2] 			//ZSCORE

   iv) q1 = df['Weight'].quantile(0.95)
       q3 = df['Weight'].quantile(0.05)
       IQR = q2-q1	
       low = q1- 1.5*IQR 
       high = q3 + 1.5*IQR        	//INTERQUARTILE RANGES

2. df.dropna()			//NULL VALUES REMOVAL
3. df.['Weight'].clip(min,max,inplace = True)   //CAPPING VALUES
4. std = df[(df['Weight']<=max) | (df['Weight']>=min)]     //INPUTING STANDARD VALUES


Week- 5 :

from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression
import numpy as np
n_splits = 5
mse_scores_cv = []
kf = KFold(n_splits=n_splits)
for train_index, test_index in kf.split(X):
  X_train_cv, X_test_cv = X[train_index], X[test_index]
  y_train_cv, y_test_cv = y[train_index], y[test_index]
  model_cv = LinearRegression()
  model_cv.fit(X_train_cv, y_train_cv)
  y_pred_cv = model_cv.predict(X_test_cv)
  mse_cv = mean_squared_error(y_test_cv, y_pred_cv)
  mse_scores_cv.append(mse_cv)

avg_mse_CV = np.mean(mse_scores_cv)
print("K-Fold Cross Validation MSE:", avg_mse_CV)

week-5 txt:

import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, KFold, ShuffleSplit
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt

# Load the Lower Back Pain Symptoms dataset
data = pd.read_csv("/content/drive/MyDrive/spine.csv")

# Preprocessing
X = data.drop(columns=['target'])
y = data['target']

# Model Training
# 1. Holdout Method
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
clf_holdout = RandomForestClassifier(random_state=42)
clf_holdout.fit(X_train, y_train)
holdout_score = clf_holdout.score(X_test, y_test)

# 2. K-Fold Cross Validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)
clf_kfold = RandomForestClassifier(random_state=42)
kfold_scores = cross_val_score(clf_kfold, X, y, cv=kf)
mean_kfold_score = kfold_scores.mean()

# 3. Bootstrap Sampling
bs = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)
clf_bootstrap = RandomForestClassifier(random_state=42)
bootstrap_scores = cross_val_score(clf_bootstrap, X, y, cv=bs)
mean_bootstrap_score = bootstrap_scores.mean()

# Display Results
print("Holdout Method Score:", holdout_score)
print("K-Fold Cross Validation Mean Score:", mean_kfold_score)
print("Bootstrap Sampling Mean Score:", mean_bootstrap_score)

# Comparison Plot
scores = [holdout_score, mean_kfold_score, mean_bootstrap_score]
methods = ['Holdout', 'K-Fold', 'Bootstrap']

plt.figure(figsize=(8, 6))
plt.bar(methods, scores, color=['blue', 'green', 'orange'])
plt.xlabel('Method')
plt.ylabel('Accuracy')
plt.title('Comparison of Model Training Methods')
plt.ylim(0.8, 1.0)
plt.show()


Week 6:

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.metrics.cluster import contingency_matrix

# Load the Lower Back Pain Symptoms dataset
data = pd.read_csv("/content/drive/MyDrive/spine.csv")

# Preprocessing (if needed)
# Assuming no preprocessing required for simplicity

# Splitting data for supervised learning
X = data.drop(columns=['target'])



#decision tree
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score

# Load the Lower Back Pain Symptoms dataset
data = pd.read_csv("/content/drive/MyDrive/spine.csv")

# Preprocessing
X = data.drop(columns=['target'])
y = data['target']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Decision Tree Classifier with entropy
dt_classifier = DecisionTreeClassifier(criterion='entropy', random_state=42)
dt_classifier.fit(X_train, y_train)

# Plotting the decision tree with entropy values
plt.figure(figsize=(12, 8))
plot_tree(dt_classifier, feature_names=X.columns, class_names=np.unique(y), filled=True, fontsize=10)
plt.show()

# Evaluate the Decision Tree Classifier
dt_predictions = dt_classifier.predict(X_test)
dt_accuracy = accuracy_score(y_test, dt_predictions)
print("Decision Tree Accuracy:", dt_accuracy)


y = data['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 1. Supervised Learning - Classification
# Training a Random Forest classifier
clf = RandomForestClassifier(random_state=42)
clf.fit(X_train, y_train)

# Evaluating the classifier
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print("Supervised Learning - Classification Metrics:")
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)

# 2. Supervised Learning - Regression
# Not applicable for this dataset

# 3. Unsupervised Learning - Clustering
# Using K-means clustering
kmeans = KMeans(n_clusters=2, random_state=42)
kmeans.fit(X)

# Evaluate cluster quality
silhouette = silhouette_score(X, kmeans.labels_)
contingency = contingency_matrix(y, kmeans.labels_)
purity = np.sum(np.amax(contingency, axis=0)) / np.sum(contingency)

print("\nUnsupervised Learning - Clustering Metrics:")
print("Purity:", purity)
print("Silhouette Width:", silhouette)
